{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe17c3cf-0988-496a-832b-c957daf961f2",
   "metadata": {},
   "source": [
    "# Test Openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225bd3e3-4c49-4b43-9c42-a2a5433b94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e20db-9b75-455c-8101-d51aad816e78",
   "metadata": {},
   "source": [
    "## 1. What is OpenAI API?\n",
    "This OpenAI API has been designed to provide developers with seamless access to state of art, pre_trained,artificial Intelligence models like gpt-3 gpt-4 dall e whisper, embeddings etc so by using this openai api you can integrate cutting edge ai capabilities into your applications regardless the programming language. <br>\n",
    "So, the conclusion is by using this OpenAI API you can unlock the advance functionalities and you can enhance the Intelligence and performance of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b6f2ec-570f-41a4-b607-6cf1a4d3cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cdb891-f40d-4494-b3d8-29121ab1b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=mykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba8f524-2b27-494e-827d-0da3b844730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = openai.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40992b6c-7337-4327-aba2-278c919551a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'),\n",
       " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'),\n",
       " Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system'),\n",
       " Model(id='o1-preview', created=1725648897, object='model', owned_by='system'),\n",
       " Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system'),\n",
       " Model(id='o1-mini', created=1725649008, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'),\n",
       " Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'),\n",
       " Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.5-preview', created=1740623059, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.5-preview-2025-02-27', created=1740623304, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system'),\n",
       " Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4693818c-333b-4f38-abab-d984454e2d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>object</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, text-embedding-ada-002)</td>\n",
       "      <td>(created, 1671217299)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, whisper-1)</td>\n",
       "      <td>(created, 1677532384)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, gpt-3.5-turbo)</td>\n",
       "      <td>(created, 1677610602)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, tts-1)</td>\n",
       "      <td>(created, 1681940951)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
       "      <td>(created, 1683758102)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(id, davinci-002)</td>\n",
       "      <td>(created, 1692634301)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(id, babbage-002)</td>\n",
       "      <td>(created, 1692634615)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct)</td>\n",
       "      <td>(created, 1692901427)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct-0914)</td>\n",
       "      <td>(created, 1694122472)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(id, dall-e-3)</td>\n",
       "      <td>(created, 1698785189)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(id, dall-e-2)</td>\n",
       "      <td>(created, 1698798177)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(id, gpt-3.5-turbo-1106)</td>\n",
       "      <td>(created, 1698959748)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(id, tts-1-hd)</td>\n",
       "      <td>(created, 1699046015)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(id, tts-1-1106)</td>\n",
       "      <td>(created, 1699053241)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(id, tts-1-hd-1106)</td>\n",
       "      <td>(created, 1699053533)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(id, text-embedding-3-small)</td>\n",
       "      <td>(created, 1705948997)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(id, text-embedding-3-large)</td>\n",
       "      <td>(created, 1705953180)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(id, gpt-3.5-turbo-0125)</td>\n",
       "      <td>(created, 1706048358)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(id, gpt-4o)</td>\n",
       "      <td>(created, 1715367049)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(id, gpt-4o-2024-05-13)</td>\n",
       "      <td>(created, 1715368132)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(id, gpt-4o-mini-2024-07-18)</td>\n",
       "      <td>(created, 1721172717)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(id, gpt-4o-mini)</td>\n",
       "      <td>(created, 1721172741)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(id, gpt-4o-2024-08-06)</td>\n",
       "      <td>(created, 1722814719)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(id, o1-preview-2024-09-12)</td>\n",
       "      <td>(created, 1725648865)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(id, o1-preview)</td>\n",
       "      <td>(created, 1725648897)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(id, o1-mini-2024-09-12)</td>\n",
       "      <td>(created, 1725648979)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(id, o1-mini)</td>\n",
       "      <td>(created, 1725649008)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(id, gpt-4o-audio-preview-2024-10-01)</td>\n",
       "      <td>(created, 1727389042)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(id, gpt-4o-audio-preview)</td>\n",
       "      <td>(created, 1727460443)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(id, omni-moderation-latest)</td>\n",
       "      <td>(created, 1731689265)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(id, omni-moderation-2024-09-26)</td>\n",
       "      <td>(created, 1732734466)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(id, gpt-4o-audio-preview-2024-12-17)</td>\n",
       "      <td>(created, 1734034239)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(id, gpt-4o-mini-audio-preview-2024-12-17)</td>\n",
       "      <td>(created, 1734115920)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(id, gpt-4o-mini-audio-preview)</td>\n",
       "      <td>(created, 1734387424)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(id, gpt-4o-2024-11-20)</td>\n",
       "      <td>(created, 1739331543)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(id, gpt-4.5-preview)</td>\n",
       "      <td>(created, 1740623059)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(id, gpt-4.5-preview-2025-02-27)</td>\n",
       "      <td>(created, 1740623304)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(id, gpt-4o-search-preview-2025-03-11)</td>\n",
       "      <td>(created, 1741388170)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(id, gpt-4o-search-preview)</td>\n",
       "      <td>(created, 1741388720)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(id, gpt-4o-mini-search-preview-2025-03-11)</td>\n",
       "      <td>(created, 1741390858)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(id, gpt-4o-mini-search-preview)</td>\n",
       "      <td>(created, 1741391161)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(id, gpt-4o-transcribe)</td>\n",
       "      <td>(created, 1742068463)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(id, gpt-4o-mini-transcribe)</td>\n",
       "      <td>(created, 1742068596)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(id, gpt-4o-mini-tts)</td>\n",
       "      <td>(created, 1742403959)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(id, gpt-4.1-2025-04-14)</td>\n",
       "      <td>(created, 1744315746)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(id, gpt-4.1)</td>\n",
       "      <td>(created, 1744316542)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(id, gpt-4.1-mini-2025-04-14)</td>\n",
       "      <td>(created, 1744317547)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(id, gpt-4.1-mini)</td>\n",
       "      <td>(created, 1744318173)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(id, gpt-4.1-nano-2025-04-14)</td>\n",
       "      <td>(created, 1744321025)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(id, gpt-4.1-nano)</td>\n",
       "      <td>(created, 1744321707)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>(id, gpt-image-1)</td>\n",
       "      <td>(created, 1745517030)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>(id, gpt-4o-audio-preview-2025-06-03)</td>\n",
       "      <td>(created, 1748908498)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id                created  \\\n",
       "0                  (id, text-embedding-ada-002)  (created, 1671217299)   \n",
       "1                               (id, whisper-1)  (created, 1677532384)   \n",
       "2                           (id, gpt-3.5-turbo)  (created, 1677610602)   \n",
       "3                                   (id, tts-1)  (created, 1681940951)   \n",
       "4                       (id, gpt-3.5-turbo-16k)  (created, 1683758102)   \n",
       "5                             (id, davinci-002)  (created, 1692634301)   \n",
       "6                             (id, babbage-002)  (created, 1692634615)   \n",
       "7                  (id, gpt-3.5-turbo-instruct)  (created, 1692901427)   \n",
       "8             (id, gpt-3.5-turbo-instruct-0914)  (created, 1694122472)   \n",
       "9                                (id, dall-e-3)  (created, 1698785189)   \n",
       "10                               (id, dall-e-2)  (created, 1698798177)   \n",
       "11                     (id, gpt-3.5-turbo-1106)  (created, 1698959748)   \n",
       "12                               (id, tts-1-hd)  (created, 1699046015)   \n",
       "13                             (id, tts-1-1106)  (created, 1699053241)   \n",
       "14                          (id, tts-1-hd-1106)  (created, 1699053533)   \n",
       "15                 (id, text-embedding-3-small)  (created, 1705948997)   \n",
       "16                 (id, text-embedding-3-large)  (created, 1705953180)   \n",
       "17                     (id, gpt-3.5-turbo-0125)  (created, 1706048358)   \n",
       "18                                 (id, gpt-4o)  (created, 1715367049)   \n",
       "19                      (id, gpt-4o-2024-05-13)  (created, 1715368132)   \n",
       "20                 (id, gpt-4o-mini-2024-07-18)  (created, 1721172717)   \n",
       "21                            (id, gpt-4o-mini)  (created, 1721172741)   \n",
       "22                      (id, gpt-4o-2024-08-06)  (created, 1722814719)   \n",
       "23                  (id, o1-preview-2024-09-12)  (created, 1725648865)   \n",
       "24                             (id, o1-preview)  (created, 1725648897)   \n",
       "25                     (id, o1-mini-2024-09-12)  (created, 1725648979)   \n",
       "26                                (id, o1-mini)  (created, 1725649008)   \n",
       "27        (id, gpt-4o-audio-preview-2024-10-01)  (created, 1727389042)   \n",
       "28                   (id, gpt-4o-audio-preview)  (created, 1727460443)   \n",
       "29                 (id, omni-moderation-latest)  (created, 1731689265)   \n",
       "30             (id, omni-moderation-2024-09-26)  (created, 1732734466)   \n",
       "31        (id, gpt-4o-audio-preview-2024-12-17)  (created, 1734034239)   \n",
       "32   (id, gpt-4o-mini-audio-preview-2024-12-17)  (created, 1734115920)   \n",
       "33              (id, gpt-4o-mini-audio-preview)  (created, 1734387424)   \n",
       "34                      (id, gpt-4o-2024-11-20)  (created, 1739331543)   \n",
       "35                        (id, gpt-4.5-preview)  (created, 1740623059)   \n",
       "36             (id, gpt-4.5-preview-2025-02-27)  (created, 1740623304)   \n",
       "37       (id, gpt-4o-search-preview-2025-03-11)  (created, 1741388170)   \n",
       "38                  (id, gpt-4o-search-preview)  (created, 1741388720)   \n",
       "39  (id, gpt-4o-mini-search-preview-2025-03-11)  (created, 1741390858)   \n",
       "40             (id, gpt-4o-mini-search-preview)  (created, 1741391161)   \n",
       "41                      (id, gpt-4o-transcribe)  (created, 1742068463)   \n",
       "42                 (id, gpt-4o-mini-transcribe)  (created, 1742068596)   \n",
       "43                        (id, gpt-4o-mini-tts)  (created, 1742403959)   \n",
       "44                     (id, gpt-4.1-2025-04-14)  (created, 1744315746)   \n",
       "45                                (id, gpt-4.1)  (created, 1744316542)   \n",
       "46                (id, gpt-4.1-mini-2025-04-14)  (created, 1744317547)   \n",
       "47                           (id, gpt-4.1-mini)  (created, 1744318173)   \n",
       "48                (id, gpt-4.1-nano-2025-04-14)  (created, 1744321025)   \n",
       "49                           (id, gpt-4.1-nano)  (created, 1744321707)   \n",
       "50                            (id, gpt-image-1)  (created, 1745517030)   \n",
       "51        (id, gpt-4o-audio-preview-2025-06-03)  (created, 1748908498)   \n",
       "\n",
       "             object                     owned_by  \n",
       "0   (object, model)  (owned_by, openai-internal)  \n",
       "1   (object, model)  (owned_by, openai-internal)  \n",
       "2   (object, model)           (owned_by, openai)  \n",
       "3   (object, model)  (owned_by, openai-internal)  \n",
       "4   (object, model)  (owned_by, openai-internal)  \n",
       "5   (object, model)           (owned_by, system)  \n",
       "6   (object, model)           (owned_by, system)  \n",
       "7   (object, model)           (owned_by, system)  \n",
       "8   (object, model)           (owned_by, system)  \n",
       "9   (object, model)           (owned_by, system)  \n",
       "10  (object, model)           (owned_by, system)  \n",
       "11  (object, model)           (owned_by, system)  \n",
       "12  (object, model)           (owned_by, system)  \n",
       "13  (object, model)           (owned_by, system)  \n",
       "14  (object, model)           (owned_by, system)  \n",
       "15  (object, model)           (owned_by, system)  \n",
       "16  (object, model)           (owned_by, system)  \n",
       "17  (object, model)           (owned_by, system)  \n",
       "18  (object, model)           (owned_by, system)  \n",
       "19  (object, model)           (owned_by, system)  \n",
       "20  (object, model)           (owned_by, system)  \n",
       "21  (object, model)           (owned_by, system)  \n",
       "22  (object, model)           (owned_by, system)  \n",
       "23  (object, model)           (owned_by, system)  \n",
       "24  (object, model)           (owned_by, system)  \n",
       "25  (object, model)           (owned_by, system)  \n",
       "26  (object, model)           (owned_by, system)  \n",
       "27  (object, model)           (owned_by, system)  \n",
       "28  (object, model)           (owned_by, system)  \n",
       "29  (object, model)           (owned_by, system)  \n",
       "30  (object, model)           (owned_by, system)  \n",
       "31  (object, model)           (owned_by, system)  \n",
       "32  (object, model)           (owned_by, system)  \n",
       "33  (object, model)           (owned_by, system)  \n",
       "34  (object, model)           (owned_by, system)  \n",
       "35  (object, model)           (owned_by, system)  \n",
       "36  (object, model)           (owned_by, system)  \n",
       "37  (object, model)           (owned_by, system)  \n",
       "38  (object, model)           (owned_by, system)  \n",
       "39  (object, model)           (owned_by, system)  \n",
       "40  (object, model)           (owned_by, system)  \n",
       "41  (object, model)           (owned_by, system)  \n",
       "42  (object, model)           (owned_by, system)  \n",
       "43  (object, model)           (owned_by, system)  \n",
       "44  (object, model)           (owned_by, system)  \n",
       "45  (object, model)           (owned_by, system)  \n",
       "46  (object, model)           (owned_by, system)  \n",
       "47  (object, model)           (owned_by, system)  \n",
       "48  (object, model)           (owned_by, system)  \n",
       "49  (object, model)           (owned_by, system)  \n",
       "50  (object, model)           (owned_by, system)  \n",
       "51  (object, model)           (owned_by, system)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(list(all_models), columns=[\"id\", \"created\", \"object\", \"owned_by\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8171cd5d-2194-4365-bfdc-41cfd682bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de5f77c-a1e1-45bb-a416-ad6b291fcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4.1\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "            # \"content\": \"How i can make a money?\"  # 0 short prompt\n",
    "#         }\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45229743-ca42-4bb4-a1b5-bc8cdcc475b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91382786-cbcd-43ae-bd98-de49a673de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let try to understand the different parameters inside the methods\n",
    "# model = \"\"\n",
    "# prompt = imput prompt\n",
    "# max_tokens = in how many number of tokens you want result\n",
    "# temperature = for getting some creative output\n",
    "# n = number of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d2181b-2380-4cfc-8c32-2c1a50d7e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b2b8c29-91be-4a65-8de1-bec928edde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f7e23-94c2-49ef-a4cb-fea0bbb681a7",
   "metadata": {},
   "source": [
    "# testopenai and langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38c24275-9fcd-4548-8453-6343cc286a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description = \"Haroon Rasheed is a student of computer science from GCUF Faisalabad. He is a Pakistani and has a 3.2 CGPA. Haroon is known his programming skills. is an active member of the university's AI club.He hope to pursue a career in artificial intelligence ahter graduation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca56cfb1-0d0d-4003-ab8f-f6164700af35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Haroon Rasheed is a student of computer science from GCUF Faisalabad. He is a Pakistani and has a 3.2 CGPA. Haroon is known his programming skills. is an active member of the university's AI club.He hope to pursue a career in artificial intelligence ahter graduation.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76dc0e29-9d40-4305-b5d1-8a43b04efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A simple prompt to extract information from \"student_description\" in a json format.     few short prompt\n",
    "# prompt = f'''\n",
    "# please extract the following information form the given text and return it as a JSON object:\n",
    "\n",
    "# name\n",
    "# university\n",
    "# grades\n",
    "# club\n",
    "\n",
    "# This is the body of text to extract the information from:\n",
    "# {student_description}\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195670e5-a139-40cc-ba01-0e07ac2315d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91355ac1-4fc2-44fc-aabb-618d496867ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "            # \"content\": prompt\n",
    "#         }\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9fc8a04-d67c-4fda-8a24-c5eefce3ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91343ca-70eb-426d-8f6f-f125e68c98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# json.loads(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476c84d-ec59-4367-8578-6713ec3ee356",
   "metadata": {},
   "source": [
    "## Function Call\n",
    "\n",
    "Learn how to connect large language models to external tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "913f49ad-231a-4649-b13d-559170393a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_custom_function=[\n",
    "#     {\n",
    "#         \"name\": \"extract_student_info\",\n",
    "#         \"description\": \"Get the student information from the body of the input text\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"name\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"Name of the person\"\n",
    "#             },\n",
    "#             \"university\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"the university name.\"\n",
    "#             },\n",
    "#             \"grades\": {\n",
    "#               \"type\": \"integer\",\n",
    "#               \"description\": \"CGPA of the student.\"\n",
    "#             },\n",
    "#             \"club\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"University club for the extracurricular activities.\"\n",
    "#             }\n",
    "#           }\n",
    "#         }\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec02206a-efee-44f6-b47c-353795224065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2 = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[{\"role\": \"user\",\"content\": prompt}],\n",
    "#     functions= student_custom_function\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef93696d-aca7-4edd-811c-f24d616cfd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1624128d-d1c3-4779-b6a8-12d780dad293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33c320-0a08-44e4-8ab1-2cfa09e8bad4",
   "metadata": {},
   "source": [
    "### pass description two or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5685934-dbbb-4cd7-bae6-25e028e36ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description_two = \"Maheen Akhtar is a student of computer science from GCUL Lahore. He is a Pakistani and has a 3.6 CGPA. Haroon is known his programming skills. is an active member of the university's MLops club.He hope to pursue a career in artificial intelligence ahter graduation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9bfcf76-f2be-4a31-8cc0-56f8494b8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_info = [student_description,student_description_two]\n",
    "# for student in student_info:\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\",\"content\": prompt}],\n",
    "    #     functions= student_custom_function,\n",
    "    #     function_call = 'auto'\n",
    "    # )\n",
    "\n",
    "    # response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    # print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659379cd-d0fb-4c7f-b3c2-b9e3c5e08473",
   "metadata": {},
   "source": [
    "### create two or more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ba71a53-2513-4103-8c0c-5817af4b487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_custom_function_two=[\n",
    "#     {\n",
    "#         \"name\": \"extract_student_info\",\n",
    "#         \"description\": \"Get the student information from the body of the input text\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"name\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"Name of the person\"\n",
    "#             },\n",
    "#             \"university\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"the university name.\"\n",
    "#             },\n",
    "#             \"grades\": {\n",
    "#               \"type\": \"integer\",\n",
    "#               \"description\": \"CGPA of the student.\"\n",
    "#             },\n",
    "#             \"club\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"University club for the extracurricular activities.\"\n",
    "#             }\n",
    "#           }\n",
    "#         }\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "925b2002-bd60-44c1-ac9d-ee1868e8029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions = [student_custom_function[0], student_custom_function_two[0]]\n",
    "# student_info = [student_description,student_description_two]\n",
    "# for student in student_info:\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\",\"content\": prompt}],\n",
    "    #     functions= functions,\n",
    "    #     function_call = 'auto'\n",
    "    # )\n",
    "\n",
    "    # response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    # print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12e535-eb5d-4663-ac8e-94f4b2afe33c",
   "metadata": {},
   "source": [
    "## advance example of function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9083f525-8a1d-4327-91ab-beae1219c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_descriptions=[\n",
    "#     {\n",
    "#         \"name\": \"get_flight_info\",\n",
    "#         \"description\": \"Get flight information between two locations\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"loc_origin\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"the departure airport, e.g. ISM\"\n",
    "#             },\n",
    "#             \"loc_destination\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"the destination airport, e.g. LHR\"\n",
    "#             },\n",
    "#           },\n",
    "#             \"required\": [\"loc_origin\", \"loc_destination\"],\n",
    "#           },\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "042d8a3b-4f47-424a-84db-89bb8c78dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt = \"When's the next flight from islamabad to lahore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db063c7c-6c89-441c-ab3f-0f0c6c5d56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2 = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#     {\n",
    "#        \"role\": \"user\",\n",
    "#        \"content\": user_prompt\n",
    "#     }\n",
    "#       ],\n",
    "#      functions =  function_descriptions,\n",
    "#      function_call = 'auto'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f40a6dcf-8f5b-47ca-9efc-7ddbb6b48ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf4a5389-94d4-4824-88cb-f7a6e0321fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14f815ad-f07f-46f9-a1c4-0f12dbfaa0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# def get_flight_info(loc_origin, loc_destination):\n",
    "#     # Example output returned fromon API or database\n",
    "#     flight_info = {\n",
    "#         \"loc_origin\": loc_origin,\n",
    "#         \"loc_destination\": loc_destination,\n",
    "#         \"datetime\": str(datetime.now() + timedelta(hours=2)),\n",
    "#         \"airline\": \"PIA\",\n",
    "#         \"flight\": \"P642\",\n",
    "#     }\n",
    "\n",
    "#     return json.dumps(flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e8bd067-131b-4d14-b6b7-88fa8885f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6af3229e-f3fc-443d-8960-4b1c5a3357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")\n",
    "# destination = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdc5fc7b-c652-49d7-b7ee-8061efb3dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5b0a474-5b5b-4a57-9426-56cb4499e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_function = eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59f0f88a-d647-4bc2-ac97-8709424c4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight = chosen_function(**params)\n",
    "\n",
    "# print(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "033b43b1-f718-4f31-a8ab-28375be0ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response3 = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#     {\"role\": \"user\",\"content\": user_prompt},\n",
    "#     {\"role\": \"function\", \"name\": response2.choices[0].message.function_call.name, \"content\": flight}\n",
    "#       ],\n",
    "#      functions =  function_descriptions,\n",
    "#      function_call = 'auto'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82a1c688-2a35-479b-be9f-30197046ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response3.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385b4ba-a439-449d-8255-654f935f605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e191e90d-86a4-470c-8cb1-7bab47fab19e",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32eaf50f-1f4a-40c7-bbda-117509a4bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0c23396-ec32-4995-b73a-eb355c6139d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fee92d1b-08bb-41c1-870a-c6c76eb6c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z\\AppData\\Local\\Temp\\ipykernel_1264\\43367455.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  client = OpenAI(openai_api_key=mykey)\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "060f212f-884b-4f22-b1e2-ccddd4475302",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"can you tell me total number of country in aisa?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ea6f72b-0aa0-495c-8030-e078b168fb38",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     emit_warning()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1356\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1353\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, stop: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   1354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1355\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(text, stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     emit_warning()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1314\u001b[0m     )\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1318\u001b[0m         [prompt],\n\u001b[0;32m   1319\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1320\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1321\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m   1322\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1324\u001b[0m     )\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1327\u001b[0m )\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:973\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    960\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    972\u001b[0m     ]\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    974\u001b[0m         prompts,\n\u001b[0;32m    975\u001b[0m         stop,\n\u001b[0;32m    976\u001b[0m         run_managers,\n\u001b[0;32m    977\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    981\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    982\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    983\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    991\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    793\u001b[0m                 prompts,\n\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    798\u001b[0m             )\n\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_community\\llms\\openai.py:463\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    452\u001b[0m         {\n\u001b[0;32m    453\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m         }\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\langchain_community\\llms\\openai.py:121\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\openai\\resources\\completions.py:541\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    540\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\openai\\_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\openai\\_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "client.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7c316-fb98-4e25-ae3c-3449fb7d6730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
